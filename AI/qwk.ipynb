{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "# Initialize Mistral AI with API Key\n",
    "MISTRAL_API_KEY = \"APIKEY\"\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0, api_key=MISTRAL_API_KEY)\n",
    "\n",
    "# Initialize Embedding Model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "# ðŸ”¹ Assign Dynamic Weights to Evaluation Traits\n",
    "def assign_trait_weights(question):\n",
    "    \"\"\"Uses Mistral AI to dynamically assign weightage to grading traits based on question complexity.\"\"\"\n",
    "\n",
    "    weightage_prompt = (\n",
    "        \"You are an expert evaluator. Analyze the given question and assign appropriate weightage (out of 100%) \"\n",
    "        \"to the following grading traits:\\n\"\n",
    "        \"1. **Content Accuracy**: Importance of factual correctness and completeness.\\n\"\n",
    "        \"2. **Coherence**: Importance of logical flow and clarity.\\n\"\n",
    "        \"3. **Vocabulary**: Importance of precise wording and expression.\\n\"\n",
    "        \"4. **Grammar**: Importance of correct grammar and readability.\\n\\n\"\n",
    "        \"### **STRICT OUTPUT FORMAT:**\\n\"\n",
    "        \"- Provide output **ONLY** in JSON format (without explanation).\\n\"\n",
    "        \"- Ensure the weights sum **exactly** to 100.\\n\"\n",
    "        \"- Example:\\n\"\n",
    "        '{ \"content_accuracy\": 40, \"coherence\": 30, \"vocabulary\": 20, \"grammar\": 10 }\\n'\n",
    "        \"Output should contain **no extra text**â€”only the JSON object.\"\n",
    "    )\n",
    "\n",
    "    system_message = SystemMessage(content=weightage_prompt)\n",
    "    user_message = HumanMessage(content=f\"Question: {question}\")\n",
    "\n",
    "    response = llm([system_message, user_message])\n",
    "\n",
    "    try:\n",
    "        # Extract JSON using regex\n",
    "        match = re.search(r\"\\{.*\\}\", response.content, re.DOTALL)\n",
    "        if match:\n",
    "            json_text = match.group(0)\n",
    "            weights = json.loads(json_text)\n",
    "        else:\n",
    "            raise ValueError(\"No valid JSON found in response.\")\n",
    "\n",
    "        # Normalize weights to sum to 100 if needed\n",
    "        total_weight = sum(weights.values())\n",
    "        if total_weight != 100:\n",
    "            weights = {k: round((v / total_weight) * 100, 2) for k, v in weights.items()}\n",
    "\n",
    "        return weights\n",
    "\n",
    "    except (json.JSONDecodeError, KeyError, TypeError, ValueError) as e:\n",
    "        print(f\"Error parsing weights: {e}\")\n",
    "        print(f\"LLM Response: {response.content}\")\n",
    "        raise ValueError(\"Invalid weight format received from LLM.\")\n",
    "\n",
    "\n",
    "# ðŸ”¹ Evaluate Answer with Score Extraction\n",
    "def evaluate_answer_dynamic(question, student_answer, model_answer):\n",
    "    \"\"\"Evaluates a student's answer and returns both feedback and numerical score.\"\"\"\n",
    "\n",
    "    weights = assign_trait_weights(question)\n",
    "\n",
    "    evaluation_prompt = (\n",
    "        \"You are an expert answer evaluator. Compare the student's answer with the model answer based on:\\n\"\n",
    "        f\"1. **Content Accuracy ({weights['content_accuracy']}%)**: Factual correctness and completeness.\\n\"\n",
    "        f\"2. **Coherence ({weights['coherence']}%)**: Logical flow and clarity.\\n\"\n",
    "        f\"3. **Vocabulary ({weights['vocabulary']}%)**: Word choice and expression.\\n\"\n",
    "        f\"4. **Grammar ({weights['grammar']}%)**: Grammar and sentence structure.\\n\\n\"\n",
    "        \"### Evaluation Requirements:\\n\"\n",
    "        \"- For each category, highlight errors and provide specific improvement suggestions\\n\"\n",
    "        \"- Calculate weighted score considering the trait weights\\n\"\n",
    "        \"### Strict Output Format:\\n\"\n",
    "        \"- End your evaluation with: 'Overall Score: X.X/10' where X.X is between 0-10\\n\"\n",
    "        \"- Example: 'Overall Score: 7.5/10'\\n\"\n",
    "        \"- The score must be the last line of your response\"\n",
    "    )\n",
    "\n",
    "    system_message = SystemMessage(content=evaluation_prompt)\n",
    "    user_message = HumanMessage(\n",
    "        content=f\"Question: {question}\\n\\nModel Answer: {model_answer}\\n\\nStudent Answer: {student_answer}\"\n",
    "    )\n",
    "\n",
    "    response = llm([system_message, user_message])\n",
    "\n",
    "    # Parse numerical score from response\n",
    "    score_match = re.search(r\"Overall Score:\\s*(\\d+\\.?\\d*)\\/10\", response.content)\n",
    "    if not score_match:\n",
    "        raise ValueError(\"Failed to parse score from evaluation response\")\n",
    "\n",
    "    return {\n",
    "        \"feedback\": response.content,\n",
    "        \"score\": float(score_match.group(1))\n",
    "    }\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def calculate_qwk(model_scores, human_scores, bins=[0, 4, 7, 10]):\n",
    "    \"\"\"\n",
    "    Robust QWK calculation with comprehensive checks\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if len(model_scores) != len(human_scores):\n",
    "        raise ValueError(\"Input arrays must have the same length\")\n",
    "\n",
    "    if len(model_scores) < 5:\n",
    "        print(\"Warning: QWK requires at least 5 samples for meaningful results\")\n",
    "        return float('nan')\n",
    "\n",
    "    # Binning function with edge handling\n",
    "    def bin_scores(scores):\n",
    "        return np.digitize(scores, bins, right=True) - 1\n",
    "\n",
    "    model_binned = bin_scores(np.array(model_scores))\n",
    "    human_binned = bin_scores(np.array(human_scores))\n",
    "\n",
    "    # Check for uniform distributions\n",
    "    if len(np.unique(human_binned)) == 1 and len(np.unique(model_binned)) == 1:\n",
    "        print(\"All scores in same bin - QWK undefined\")\n",
    "        return float('nan')\n",
    "\n",
    "    # Calculate QWK with full labels\n",
    "    all_labels = np.unique(np.concatenate([model_binned, human_binned]))\n",
    "    return cohen_kappa_score(human_binned, model_binned,\n",
    "                           labels=all_labels,\n",
    "                           weights='quadratic')\n",
    "\n",
    "# Test cases\n",
    "print(calculate_qwk([7], [7]))  # Single sample: Warning + NaN\n",
    "print(calculate_qwk([7,7,7], [7,7,7]))  # Uniform scores: ðŸš¨ + NaN\n",
    "print(calculate_qwk([5,6,7,8,9], [6,7,8,9,10]))  # Realistic: ~0.6-0.8"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
